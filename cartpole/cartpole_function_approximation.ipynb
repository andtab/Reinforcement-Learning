{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cartpole-function-approximation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "XGoDjno2rYEt",
        "8kheSgI0rrqy",
        "9zkWRdA6rtsg",
        "LpXjlyp0rqK0",
        "8FE2zcWouykh",
        "OucynkHxH9Yc",
        "4dfHgwVqfwpy",
        "fxpfF98F-6oB",
        "6Zfdo8AggEkE",
        "aH9BuxzVqCzE",
        "9tCEdivZ84he"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YE9O_eJlkfc"
      },
      "source": [
        "#**I. Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGd0UykIar3T"
      },
      "source": [
        "In the previous explorations of Reinforcement Learning concepts embarked upon throughout this course, value functions and policies were stored in a tabular format. This is manageable when the 𝓢 x 𝓐 space, or environment, of consideration is relatively small and can be discretized. Practically speaking however, this approach breaks down when the 𝓢 x 𝓐 space increases in size. Tables become inefficient. Consequently, this approach must be abandoned in favour of one where tables are no longer needed, and the constraints associated with them can be relaxed. With that goal in mind, this is where Function Approximation comes into play. The aim is to have our agent explore its environment such that it will learn a function which will approximate its action value function. The function will be learned through use of a simple Neural Network (NNs), as this notebook will go on to demonstrate. This time around, the CartPole environment will be the subject of investigation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fvbwj589lffj"
      },
      "source": [
        "#**II. Imports and Installations** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl3kXuEiuE4j"
      },
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvkuIXztuHuL"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMKC6CtNuJNC"
      },
      "source": [
        "# Standard imports for 'gym' based RL projects\n",
        "import gym\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "# Imports from Exercise IV\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Additional imports required for this implementation\n",
        "import copy \n",
        "import random"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZKDWGgauLKW",
        "outputId": "8c198912-20c0-415d-eec8-ab53cbbc4c0d"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7ff23d8f6290>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kisR0I3wkCTO"
      },
      "source": [
        "#**III. Randomly Acting Cartpole**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQ2GRp8KkJ5y"
      },
      "source": [
        "As usual, the performance of the randomly acting agent will be exhibited for us to compare against by the time Reinforcement Learning theory has been applied to this problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "SObF2Hd4tvis",
        "outputId": "6f8595fc-1d5c-4c50-dcbc-31637aa0f377"
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "env.reset()\n",
        "prev_screen = env.render(mode='rgb_array')\n",
        "plt.imshow(prev_screen)\n",
        "\n",
        "for i in range(50000):\n",
        "  action = env.action_space.sample()\n",
        "  print(\"step i\",i,\"action=\",action)\n",
        "  print(env.env.state)\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  print(\"obs=\",obs,\"reward=\",reward,\"done=\",done,\"info=\",info)\n",
        "  screen = env.render(mode='rgb_array')\n",
        "  \n",
        "  plt.imshow(screen)\n",
        "  ipythondisplay.clear_output(wait=True)\n",
        "  ipythondisplay.display(plt.gcf())\n",
        "\n",
        "  if done:\n",
        "    break\n",
        "    \n",
        "ipythondisplay.clear_output(wait=True)\n",
        "env.close()\n",
        "print(\"Iterations that were run:\", i + 1)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iterations that were run: 11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GgdflnoGf5s"
      },
      "source": [
        "#**IV. Helper Functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dklwpeNofENu"
      },
      "source": [
        "Below are found some helper functions which will help simplify readability of RL theory implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZodBkswWtYX-"
      },
      "source": [
        "##**2. Policy Helper Functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OucynkHxH9Yc"
      },
      "source": [
        "###ε-Greedy Policy State Update"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EN-lbi1H7n5"
      },
      "source": [
        "# Updates ε-greedy policy for a specified state (i.e. calculate π(s))\n",
        "def eps_greedy_policy_update(input_policy_state, eps = 0.1):\n",
        "\n",
        "  # Determine the amount of actions that can be taken to go from current\n",
        "  # state to another state (|𝒜(s)|)\n",
        "  act_set_size = len(input_policy_state)\n",
        "\n",
        "  # Determine the maximum probability of taking an action for a given state\n",
        "  optimal_value = max(input_policy_state.values())\n",
        "\n",
        "  # Determine which actions out of the state's action set |𝒜(s)|\n",
        "  # are optimal, based on the calculated optimal value\n",
        "  optimal_actions = [action for action in input_policy_state if input_policy_state[action] == optimal_value]     \n",
        "\n",
        "  # Initialize sub dictionary of the action set for the soft policy\n",
        "  soft_policy_action_set = {}\n",
        "    \n",
        "  # Loop through each action possible in the current state\n",
        "  for action in input_policy_state:\n",
        "    \n",
        "    # The following if statements intend to assign probabilities of taking\n",
        "    # actions in given states in accordance with the ε-soft requirements\n",
        "    if action in optimal_actions:\n",
        "\n",
        "      if len(optimal_actions) != act_set_size:\n",
        "        # When the optimal value in an action set is > 0\n",
        "        # When all actions for a given state are not equiprobable,\n",
        "        # Set the probability of taking the optimal action to be = (1/|𝒜'(s)|)(1 - ε) + ε/|𝒜(s)|\n",
        "        # Where |𝒜'(s)| represents the number of optimal actions within |𝒜(s)| (i.e. ties)\n",
        "        soft_policy_action_set[action] = (1/len(optimal_actions))*(1 - eps) + eps/act_set_size\n",
        "\n",
        "      else:\n",
        "        # Otherwise, if all actions in |𝒜(s)| are equiprobable,\n",
        "        # and not equal to 0, they shall remain equiprobable\n",
        "        soft_policy_action_set[action] = (1/len(optimal_actions))\n",
        "\n",
        "    else:\n",
        "      # For non optimal actions, set the probability of taking\n",
        "      # these actions = ε/|𝒜(s)|\n",
        "      soft_policy_action_set[action] = eps/act_set_size\n",
        "\n",
        "  # Return the ε policy for a given state\n",
        "  return soft_policy_action_set"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8nt6bDYoj1p"
      },
      "source": [
        "##**3. Function Approximation Helper Functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH9BuxzVqCzE"
      },
      "source": [
        "###Neural Network Input Size Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w8qo-evt9QV"
      },
      "source": [
        "# This is a function used to determine the input size required for the Neural\n",
        "# Network (NN) based on the size of the state tuple and action input combined\n",
        "def NN_input(environment):\n",
        "\n",
        "  # Calculate size of state tuple for environment\n",
        "  state_tuple_size = environment.env.observation_space.shape[0] \n",
        "  \n",
        "  # Calculate action input size \n",
        "  action_input_size = 1\n",
        "\n",
        "  # Calculate input size requireed for the model\n",
        "  model_input_size = state_tuple_size + action_input_size\n",
        "\n",
        "  # Return the model's required input dimension\n",
        "  return model_input_size"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pboNkcOiotcX"
      },
      "source": [
        "###Neural Network Output Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CROSnxLC7T77"
      },
      "source": [
        "# The following functon takes the current state (described by a tuple of \n",
        "# variables) and the selected action at this state (described by an integer),\n",
        "# combines them into an input for the Neural Network (NN) being used for \n",
        "# function approximation. The output of the NN, returned by this function,\n",
        "# is the approximated action value for the given state-action pair. \n",
        "def NN_output(state, action, model):\n",
        "\n",
        "  # Append the state tuple and action into a single array for input into the \n",
        "  # neural network\n",
        "  state_action_input = np.append([np.array(state)], [[action]], axis = 1)\n",
        "\n",
        "  # Store the output of the model into the 'output variable'\n",
        "  output = model(state_action_input)\n",
        "\n",
        "  # Return the model's output\n",
        "  return output"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZpi0F57_dtN"
      },
      "source": [
        "###Reset Model Weights to Previous Value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLBZZS7I_dUv"
      },
      "source": [
        "# Function to reset model weights; use prior to running algorithm, given that\n",
        "# the current run and former runs are executed in the same runtime\n",
        "# The weights to which the model must be reset will need to be specified\n",
        "def reset_weights(input_model, initial_weights):\n",
        "  input_model.set_weights(initial_weights)\n",
        "\n",
        "  # Return model with weights reset\n",
        "  return input_model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FV_D4yChhnAT"
      },
      "source": [
        "###Custom Weight Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEZzwTrsXa8U"
      },
      "source": [
        "# Function to quickly reinitialize model weights whenever required - as \n",
        "# implemented currently, weights can either be initialized to 0 or to the \n",
        "# form of a Gaussian distribution for a specified mean and standard deviation\n",
        "def initialize_model_weights(model, initialization = 'gaussian', mean = 0, std_dev = 1):\n",
        "\n",
        "  # variable for error handling unsupportable inputs for the 'initialization\n",
        "  # parameter\n",
        "  initialization_error = False\n",
        "\n",
        "  # Initialize list to store the weights to apply to the \n",
        "  # model\n",
        "  weights_list = []\n",
        "\n",
        "  # The following for loop will leave biases untouched\n",
        "  # and will update weights such that they follow a Gaussian \n",
        "  # distribution of values with mean and standard deviation\n",
        "  # as specified by the 'loc' and 'scale' parameters of the \n",
        "  # 'np.random.normal' function\n",
        "  for x in range(0,len(model.weights)):\n",
        "    \n",
        "    # Identifying the term 'kernel' in the model name will \n",
        "    # indicate that the following entry in the current \n",
        "    # 'model.weights' list represents the weights, and not\n",
        "    # the bias of a particular layer\n",
        "    if 'kernel' in model.weights[x].name:\n",
        "\n",
        "      # Determine the appropriate shape for the updated\n",
        "      # weights based on the current shape of the weights\n",
        "      weights_shape = model.get_weights()[x].shape\n",
        "\n",
        "      # update weights\n",
        "      if initialization == 'gaussian':\n",
        "        weight = np.random.normal(loc = mean, scale = std_dev, size = weights_shape)\n",
        "\n",
        "      elif initialization == 'zero':\n",
        "        weight = np.zeros(shape = weights_shape)\n",
        "\n",
        "      else:\n",
        "        print('\\'', initialization, '\\' is an invalid initialization type...' )\n",
        "        print('Choose \\'initialization\\' parameter to be \\'zeros\\' or \\'gaussian\\'')\n",
        "        break\n",
        "\n",
        "      # append to weight_list used to finally update current model \n",
        "      # weights\n",
        "      weights_list.append(weight)\n",
        "    \n",
        "    else:\n",
        "      # bias terms will be updated to a zero vector\n",
        "\n",
        "      # Determine the appropriate shape for the updated\n",
        "      # weights based on the current shape of the weights\n",
        "      bias_shape = model.get_weights()[x].shape\n",
        "\n",
        "      # update weights\n",
        "      bias = np.zeros(shape =  bias_shape)\n",
        "\n",
        "      # append to weight_list used to finally update current model \n",
        "      # weights\n",
        "      weights_list.append(bias)\n",
        "\n",
        "  # Update model weights as long as 'initialization' parameter input is valid\n",
        "\n",
        "  if initialization_error == False:\n",
        "    model.set_weights(weights_list)\n",
        "\n",
        "  # Return updated model\n",
        "  return model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC2pTBx1I95z"
      },
      "source": [
        "##**4. TD Control Helper Functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcZVW4IcwltN"
      },
      "source": [
        "###Reward Adjustment Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eytPtRiwau2"
      },
      "source": [
        "**NOTE:** The built-in reward structure of the CartPole environment provided by OpenAI gym was modified such that a greater reward is provided to the agent when the pole's inclination angle from the vertical is less than or equal to 3 degree in either direction. Also a reward of 0 will be given if the CartPole transitions to the terminal state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqrF5f4Z4DLX"
      },
      "source": [
        "# Function to modify the reward attained by the agent depending on its current\n",
        "# state\n",
        "def reward_calculation(state, reward, done):\n",
        "\n",
        "  # Extract individual variables representing the specified state\n",
        "  position, velocity, theta_rad, ang_vel_rad = state\n",
        "\n",
        "  # Convert the angle to degrees\n",
        "  theta = np.degrees(theta_rad)\n",
        "\n",
        "  # Reward increment to be applied when the agent's state meets certain criteria\n",
        "  reward_increment = 7\n",
        "\n",
        "  # For the cartpole environment, if the pole has an angle between -1 and 1 \n",
        "  # degrees from the vertical then the reward is incremented accordingly\n",
        "  if theta >= -1 and theta <= 1 and done == False:\n",
        "    reward_mod = reward + reward_increment\n",
        "\n",
        "  # If the agent has arrived at the terminal state, set the received reward to\n",
        "  # -1\n",
        "  elif done == True:\n",
        "    reward_mod = -1\n",
        "\n",
        "  # In all other cases, the reward shall remain unmodified\n",
        "  else:\n",
        "    reward_mod = reward\n",
        "\n",
        "  # Return the resulting reward\n",
        "  return reward_mod"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJXo_wiTijpj"
      },
      "source": [
        "###Episode Verdict Evaluator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__nOcaO4gmoq"
      },
      "source": [
        "# Function used to determine if an episode was successful\n",
        "def episode_verdict(state):\n",
        "\n",
        "  # Extract state variables \n",
        "  position, velocity, theta_rad, ang_vel_rad = state\n",
        "\n",
        "  # Convert theta from rad to degrees\n",
        "  theta = np.degrees(theta_rad)\n",
        "\n",
        "  # First to conditions represent a failed episode. Otherwise, successful \n",
        "  # episode\n",
        "  if position > 2.4 or position < -2.4:\n",
        "    episode_result = 0\n",
        "\n",
        "  elif theta > 12 or theta < -12:\n",
        "    episode_result = 0 \n",
        "\n",
        "  else:\n",
        "    episode_result = 1\n",
        "\n",
        "  return episode_result"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Yw9Z0MobESO"
      },
      "source": [
        "###SARSA(0) Algorithm (Based on Function Approximation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-Q1HS1NbDyV"
      },
      "source": [
        "def TD0_SARSA_FA(environment, model, error, optimizer, eps = 0.2, gamma = 0.95):\n",
        "\n",
        "  # 'abort' is used to halt training process if it is discovered that the \n",
        "  # NN based function approximation is diverging to an unhelpful result\n",
        "  # (i.e. action value predictions tending to infinity or equalizing, \n",
        "  # despite the action taken)\n",
        "  abort = False\n",
        "\n",
        "  # 'done' variable signifies if the condition for stopping an episode (excluding\n",
        "  # successfully episodes) has been met\n",
        "  done = False\n",
        "\n",
        "  # Initialize a reward list which will store rewards attained in an episode\n",
        "  reward_list = []\n",
        "  \n",
        "  # Initialize an error list which will store the average per prediction error\n",
        "  # in an episode\n",
        "  error_list = []\n",
        "\n",
        "  # Reset environment before processing episode\n",
        "  environment.reset()\n",
        "\n",
        "  # Set the initial state of the environment\n",
        "  state = environment.env.state\n",
        "\n",
        "  # Take initial episode action based on ε-greedy policy\n",
        "  action = action_selection_NN(environment, state, model, eps)\n",
        "\n",
        "  # Check to see if the neural network is providing problematic results\n",
        "  abort = divergent_NN_test(environment, state, model)\n",
        "\n",
        "  # If problematic results are observed, exit the function early\n",
        "  if abort:\n",
        "    episode_result = 0\n",
        "    return model, reward_list, episode_result, error_list, optimizer, abort\n",
        "\n",
        "  # Loop for each step of the episode\n",
        "  while done == False:\n",
        "\n",
        "    # Take the selected action, and observe the next state and resulting \n",
        "    # reward\n",
        "    obs, reward, done, info = environment.step(action)\n",
        "\n",
        "    # Store the state arrived at in a variable\n",
        "    new_state = environment.env.state\n",
        "\n",
        "    # Calculate reward, modified depending on the state to which the mountain \n",
        "    # car has transitioned - inspect the 'reward_calculation' function for greater\n",
        "    # detail\n",
        "    reward = reward_calculation(new_state, reward, done)\n",
        "\n",
        "    # Append reward to a reward list, used to summarize training\n",
        "    reward_list.append(reward)\n",
        "\n",
        "    # If the next state is terminal, update weights of the neural network \n",
        "    # approximation of action value function\n",
        "    if done:\n",
        "      with tf.GradientTape() as tape:\n",
        "\n",
        "        # Set approximated action value to the output of constructed neural\n",
        "        # network for state-action pair at timestep τ\n",
        "        q_hat = NN_output(state, action, model)\n",
        "\n",
        "        # Set per prediction error to be measured between estimated return\n",
        "        # given the current state-action pair and the neural network based\n",
        "        # action value function approximation\n",
        "        return_pred = tf.constant(reward, shape = (1, 1), dtype = float)\n",
        "\n",
        "        ppError = error(return_pred, q_hat)\n",
        "\n",
        "      # Determine gradient of function approximation\n",
        "      q_hat_grad = tape.gradient(ppError, model.trainable_weights)  \n",
        "\n",
        "      # Update model weights\n",
        "      optimizer.apply_gradients(zip(q_hat_grad, model.trainable_weights))\n",
        "\n",
        "      # Store error in list to summarize results of training  when implementing\n",
        "      # TD control\n",
        "      error_list.append(ppError.numpy())\n",
        "\n",
        "      # If terminal state is to be arrived at, exit the function\n",
        "      break\n",
        "\n",
        "    # Select new action based on new state\n",
        "    new_action = action_selection_NN(environment, new_state, model, eps)\n",
        "\n",
        "    # Check to see if the neural network is providing problematic results\n",
        "    abort = divergent_NN_test(environment, new_state, model)\n",
        "\n",
        "    # If problematic results are observed, exit the function early\n",
        "    if abort:\n",
        "      break\n",
        "\n",
        "    # Update weights of the model\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "      # Set approximated action value to the output of constructed neural\n",
        "      # network for state-action pair at timestep τ\n",
        "      q_hat_new = NN_output(new_state, new_action, model)\n",
        "      q_hat = NN_output(state, action, model)\n",
        "\n",
        "      # Set per prediction error to be measured between estimated return\n",
        "      # given the current state-action pair and the neural network based\n",
        "      # action value function approximation\n",
        "      return_pred = tf.constant(reward + gamma * q_hat_new, shape = (1, 1), dtype = float)\n",
        "\n",
        "      ppError = error(return_pred, q_hat)\n",
        "\n",
        "    # Determine gradient of function approximation\n",
        "    q_hat_grad = tape.gradient(ppError, model.trainable_weights)  \n",
        "\n",
        "    # Update model weights\n",
        "    optimizer.apply_gradients(zip(q_hat_grad, model.trainable_weights))\n",
        "\n",
        "    # Store error in list to summarize results of training  when implementing\n",
        "    # TD control\n",
        "    error_list.append(ppError.numpy())\n",
        "\n",
        "    # Update state and action\n",
        "    state, action = new_state, new_action\n",
        "\n",
        "  # Determine through 'episode_verdict' function whether or not episode is \n",
        "  # a success or failure\n",
        "  episode_result = episode_verdict(environment.env.state)\n",
        "\n",
        "  return model, reward_list, episode_result, error_list, optimizer, abort"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdaoa43F4ucY"
      },
      "source": [
        "###SARSA(n) Algorithm (Based on Function Approximation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCDkIytHm6Pd"
      },
      "source": [
        "# N-Step SARSA algorithm adapted for function approximation\n",
        "def N_STEP_SARSA_FA(environment, model, error, optimizer, n = 1, eps = 0.2, gamma = 0.95):\n",
        "\n",
        "  # 'abort' is used to halt training process if it is discovered that the \n",
        "  # NN based function approximation is diverging to an unhelpful result\n",
        "  # (i.e. action value predictions tending to infinity or equalizing, \n",
        "  # despite the action taken)\n",
        "  abort = False\n",
        "\n",
        "  # Initialize a reward list which will store rewards attained in an episode\n",
        "  reward_list = []\n",
        "  \n",
        "  # Initialize an error list which will store the average per prediction error\n",
        "  # in an episode\n",
        "  error_list = []\n",
        "  state_action_reward_list = []\n",
        "\n",
        "  # Ensure the environment is reset before processing an episode\n",
        "  environment.reset()\n",
        "\n",
        "  # generate initial state and action\n",
        "  state = environment.env.state\n",
        "\n",
        "  # Take initial episode action based on ε-greedy policy\n",
        "  action = action_selection_NN(environment, state, model, eps)\n",
        "\n",
        "  # Check to see if the neural network is providing problematic results\n",
        "  abort = divergent_NN_test(environment, state, model)\n",
        "\n",
        "  # If problematic results are observed, exit the function early\n",
        "  if abort:\n",
        "    episode_result = 0\n",
        "    return model, reward_list, episode_result, error_list, optimizer, abort\n",
        "\n",
        "  # Initialize timestep variable, 't' to zero - terminal timestep 'T'\n",
        "  # initialized to infinity - this is done to ensure that 't' will always be\n",
        "  # less than 'T', allowing for episode to be properly processed. Actual 'T'\n",
        "  # will eventually be determined if episode terminates due to 'done' condition\n",
        "  # being met\n",
        "  t = 0\n",
        "  T = float(np.inf)\n",
        "\n",
        "  while True:\n",
        "\n",
        "    # If current timestep is not the terminal step, enter the if \n",
        "    # statement to take the next action\n",
        "    if t < T:\n",
        "      \n",
        "      # Extract variables associated with taking an action\n",
        "      obs, reward, done, info = environment.step(action)\n",
        "\n",
        "      # Calculate reward, modified depending on the state to which the mountain \n",
        "      # car has transitioned - inspect the 'reward_calculation' function for greater\n",
        "      # detail\n",
        "      reward = reward_calculation(state, reward, done)\n",
        "\n",
        "      # Store initial state action and reward\n",
        "      state_action_reward_list.append((state, action, reward))\n",
        "\n",
        "      # Append to reward list used to summarize results of training when \n",
        "      # implementing TD control\n",
        "      reward_list.append(reward)\n",
        "\n",
        "      # Update 'state' variable to the state arrived at after taking \n",
        "      # 'action'\n",
        "      state = environment.env.state\n",
        "\n",
        "      # If the next state is terminal, update 'T', which represents the \n",
        "      # terminal timestep\n",
        "      if done:\n",
        "        T = t + 1\n",
        "      \n",
        "      else:\n",
        "        # Take action based on ε-greedy policy, as long as terminal state has\n",
        "        # not yet been reached\n",
        "        action = action_selection_NN(environment, state, model, eps)\n",
        "\n",
        "        # Check to see if the neural network is providing problematic results\n",
        "        abort = divergent_NN_test(environment, state, model)\n",
        "\n",
        "        # If problematic results are observed, exit the function early\n",
        "        if abort:\n",
        "          break\n",
        "\n",
        "    # tau is intended to represents timesteps for which action value \n",
        "    # updates can be made; when the quantity tau is non-negative \n",
        "    # (starting at tau = 0), the return at timestep tau can be estimated \n",
        "    # using the n steps taken by the agent in the environment\n",
        "    tau = t - n + 1\n",
        "\n",
        "    # Enter the loop only when τ (tau) is >= 0 (i.e. when enough \n",
        "    # states have been visited to estimate n-step returns)\n",
        "    if tau >= 0:\n",
        "\n",
        "      # Initialize the return variable to 0 prior to its calculation\n",
        "      G = 0\n",
        "\n",
        "      # For the return calculation, we need to check that the current \n",
        "      # timestep τ + n (number of steps after τ) is equal to or less than \n",
        "      # T - cannot calculate returns beyond T\n",
        "      end_step = min(tau + n, T)\n",
        "\n",
        "      # Calculate first component of estimate for return using n \n",
        "      # rewards achieved beyond τ\n",
        "      for i in range(tau, end_step):\n",
        "\n",
        "        # Reward achieved when transitioning from state i - 1 to i\n",
        "        reward_i = state_action_reward_list[i][2]\n",
        "\n",
        "        # Update returns computation with these n rewards and γ (gamma)\n",
        "        G += tf.constant((gamma**(i - tau)) * reward_i, shape = (1, 1), dtype = float)\n",
        "\n",
        "      # If the timestep τ + n is not the terminal timestep T, enter\n",
        "      # the if statement and update the return to estimate for the \n",
        "      # remaining terms of the return calculation\n",
        "      if tau + n < T:\n",
        "\n",
        "        # Extract the state and action stored at timestep τ + n\n",
        "        state_tau_n, action_tau_n = state, action\n",
        "\n",
        "        # Complete our estimate of the return using our estimated\n",
        "        # action value function value at the state and action for \n",
        "        # timestep τ + n\n",
        "\n",
        "        # Extract neural network output \n",
        "        q_hat_tau_n = NN_output(state_tau_n, action_tau_n, model)\n",
        "\n",
        "        # Convert neural network output from tensor to scalar value \n",
        "        q_hat_tau_n_val = q_hat_tau_n.numpy()[0][0]\n",
        "\n",
        "        # Compute return\n",
        "        G += tf.constant((gamma**n) * q_hat_tau_n_val, shape = (1, 1), dtype = float)\n",
        "\n",
        "      # Extract the state and action stored at timestep τ\n",
        "      state_tau, action_tau, _ = state_action_reward_list[tau]\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "        \n",
        "        # Set approximated action value to the output of constructed neural\n",
        "        # network for state-action pair at timestep τ\n",
        "        q_hat = NN_output(state_tau, action_tau, model)\n",
        "\n",
        "        # Set per prediction error to be measured between estimated return\n",
        "        # given the current state-action pair and the neural network based\n",
        "        # action value function approximation\n",
        "        ppError = error(G, q_hat)\n",
        "\n",
        "      # Determine gradient of function approximation\n",
        "      q_hat_grad = tape.gradient(ppError, model.trainable_weights)  \n",
        "\n",
        "      # Store error in list to summarize results of training  when implementing\n",
        "      # TD control\n",
        "      error_list.append(ppError.numpy())\n",
        "\n",
        "      # Update model weights\n",
        "      optimizer.apply_gradients(zip(q_hat_grad, model.trainable_weights))\n",
        "  \n",
        "    # If the timestep τ is the step just before reaching\n",
        "    # the terminal state, exit the algorithm\n",
        "    if tau == T - 1:\n",
        "\n",
        "      break\n",
        "\n",
        "    # Increment timestep variable t\n",
        "    t += 1\n",
        "  \n",
        "  # Determine if episode is successful or not\n",
        "  episode_result = episode_verdict(environment.env.state)\n",
        "\n",
        "  return model, reward_list, episode_result, error_list, optimizer, abort"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIujFDQ2f6eZ"
      },
      "source": [
        "def action_selection_NN(environment, state, model, eps):\n",
        "\n",
        "  # Initialize a dictionary that will represent the different action values\n",
        "  # at the current state\n",
        "  action_value_state = {}\n",
        "\n",
        "  # For each action choice in the space of action choices, determine the \n",
        "  # action value \n",
        "  for action_choice in range(environment.env.action_space.n):\n",
        "    action_value_state[action_choice] = NN_output(state, action_choice, model).numpy()[0][0]\n",
        "  \n",
        "  # From the current state, extract the possible actions and their corresponding\n",
        "  # weights, according to ε-greedy policy\n",
        "  state_policy = eps_greedy_policy_update(action_value_state, eps)\n",
        "\n",
        "  # Extract possible actions at the state\n",
        "  action_set = list(state_policy.keys())\n",
        "  \n",
        "  # Extract action weights based on \n",
        "  action_weights = list(state_policy.values())\n",
        "\n",
        "  # Probabilistically choosen an action based on each action's weight, \n",
        "  # according to ε-greedy policy\n",
        "  selected_action = random.choices(action_set, action_weights, k = 1)[0]\n",
        "\n",
        "  return selected_action"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCwU9DAC4zsd"
      },
      "source": [
        "###Divergent FA Testing Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfUq-BbX51id"
      },
      "source": [
        "# Function used to determine if Function Approximation is diverging to a \n",
        "# problematic result \n",
        "def divergent_NN_test(environment, state, model):\n",
        "  \n",
        "  # Parameter to abort action selection if the NN appears to diverge to produce\n",
        "  # problematic outputs (i.e. exactly equal action values or action values\n",
        "  # diverging to infinity)\n",
        "  abort = False\n",
        "\n",
        "  # Initialize a dictionary that will represent the different action values\n",
        "  # at the current state\n",
        "  action_value_state = {}\n",
        "\n",
        "  # For each action choice in the space of action choices, determine the \n",
        "  # action value \n",
        "  for action_choice in range(environment.env.action_space.n):\n",
        "    action_value_state[action_choice] = NN_output(state, action_choice, model).numpy()[0][0]\n",
        "  \n",
        "  # From the dictionary extract a list of only the action values\n",
        "  action_value_state_list = list(action_value_state.values())\n",
        "\n",
        "  # Use the list created to see if action values are exactly equal \n",
        "  # irregardless of the action taken at the current state\n",
        "  action_value_comparison = [1 if action_value_state_list[0] == action_value_state_list[x] else 0 for x in range(len(action_value_state_list))]\n",
        "\n",
        "  # If it is found that the action values are diverging to infinity, abort the\n",
        "  # action selection process\n",
        "  if np.isinf(action_value_state_list).any() or np.isnan(action_value_state_list).any():\n",
        "    print('>> WARNING - Training has stopped as it appears prediction action-value function is diverging to infinity')\n",
        "    print('>> Please reinitialize weights, adjust hyperparameters, and retry the algorithm')\n",
        "    abort = True\n",
        "\n",
        "  # If it is found that the action values have converged to being exactly \n",
        "  # equal irrespective of chosen action, abrot the action selection process\n",
        "  elif sum(action_value_comparison) == len(action_value_state_list):\n",
        "    print('>> WARNING - Training has stopped as it appears approximate action values have become equal regardless of action taken at state')\n",
        "    print('>> Please reinitialize weights, adjust hyperparameters, and retry the algorithm')\n",
        "    abort = True\n",
        "\n",
        "\n",
        "  return abort"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUz9qEoO32ed"
      },
      "source": [
        "###TD-Control Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOE8U3uctfy2"
      },
      "source": [
        "# TD Control Function\n",
        "def TD_CONTROL(environment, model, error, optimizer, n = 0, eps = 0.2, gamma = 0.95, episode_count = 5000, batch_printout = False):\n",
        "\n",
        "  lr_initial = optimizer.lr.numpy()\n",
        "  lr_decay = 0.01\n",
        "  minimum_lr = 3e-4\n",
        "\n",
        "  lr_update_step = 50\n",
        "\n",
        "  lr_minimum_reached = False\n",
        "\n",
        "  eps_decay = 0.99\n",
        "  minimum_eps = 0.05\n",
        "\n",
        "  eps_update_step = 50\n",
        "\n",
        "  eps_minimum_reached = False\n",
        "\n",
        "  # Parameter representing the maximum number of episodes possible in an episode\n",
        "  # DO NOT CHANGE\n",
        "  max_episode_steps = 200\n",
        "\n",
        "  # Set a batch size for which updates on training will be reported through a \n",
        "  # print statement\n",
        "  batch_size = 100\n",
        "\n",
        "  # Initialize lists that will be used to provide statistics on the training\n",
        "  # of the model\n",
        "  total_reward = []\n",
        "  total_episode_length = []\n",
        "  total_episode_result = []\n",
        "  average_episode_error_list = []\n",
        "\n",
        "  # Loop the selected method for each episode\n",
        "  for episode in range(0, episode_count):\n",
        "\n",
        "    if n > 0:\n",
        "      model, reward_list, episode_result, error_list, optimizer, abort = N_STEP_SARSA_FA(environment = environment, model = model, error = error, optimizer = optimizer, n = n, eps = eps, gamma = gamma)\n",
        "    \n",
        "    else:\n",
        "      model, reward_list, episode_result, error_list, optimizer, abort = TD0_SARSA_FA(environment = environment, model = model, error = error, optimizer = optimizer, eps = eps, gamma = gamma)\n",
        "    \n",
        "    if abort:\n",
        "      print('>> Hyperparameter Combination: lr = ', optimizer.lr.numpy(), ', gamma = ', gamma, ', eps = ', eps)\n",
        "\n",
        "      break\n",
        "\n",
        "    if eps_minimum_reached == False:\n",
        "\n",
        "      if (episode + 1) % eps_update_step == 0:\n",
        "\n",
        "        if eps > minimum_eps:\n",
        "          eps = eps * eps_decay\n",
        "\n",
        "          if eps <= minimum_eps:\n",
        "            eps = minimum_eps\n",
        "            eps_minimum_reached = True\n",
        "\n",
        "    \n",
        "    if lr_minimum_reached == False:\n",
        "\n",
        "      if (episode + 1) % lr_update_step == 0:\n",
        "        \n",
        "        if optimizer.lr > minimum_lr:\n",
        "          optimizer.lr = lr_initial * (1/(1 + lr_decay * (episode + 1)))\n",
        "\n",
        "          if optimizer.lr <= minimum_lr:\n",
        "            optimizer.lr = minimum_lr\n",
        "            lr_minimum_reached = True\n",
        "\n",
        "    # For each episode, sum up the rewards, retrieve the number of episode steps,\n",
        "    # retrieve the episode outcome (success or failure), and retrieve the \n",
        "    # average per prediction error for the episode; store these into \n",
        "    # lists which will be used to summarize overall training statistics\n",
        "    #print(reward_list)\n",
        "    total_reward.append(sum(reward_list))\n",
        "    total_episode_length.append(len(reward_list))\n",
        "    total_episode_result.append(episode_result)\n",
        "    average_episode_error_list.append(sum(error_list)/len(error_list))\n",
        "\n",
        "    # Report training statistics only when a batch is completed\n",
        "    if (episode + 1) % batch_size == 0:\n",
        "      #print(policy)\n",
        "      # Print number of episodes completed and the total number of episodes\n",
        "      # for which training will run, as specified by the 'episode_count' \n",
        "      # parameter\n",
        "\n",
        "      if batch_printout == True:\n",
        "        print(episode + 1, ' episodes completed... out of ', episode_count, '*' * 40)\n",
        "\n",
        "      # Calculate the average reward attained in episodes from the first episode\n",
        "      # to the current episode\n",
        "      total_avg_reward = sum(total_reward)/len(total_reward)\n",
        "\n",
        "      # Calculate the average episode length from the first episode\n",
        "      # to the current episode\n",
        "      total_avg_episode_length = sum(total_episode_length)/len(total_episode_length)\n",
        "\n",
        "      # Calculate the number of successful episodes from the first episode to\n",
        "      # the current episode\n",
        "      episode_result_cumulative = sum(total_episode_result)\n",
        "\n",
        "      # Calculate the average of average errors from the first episode to the\n",
        "      # current episode\n",
        "      average_episode_error_cumulative = sum(average_episode_error_list)/len(average_episode_error_list)\n",
        "\n",
        "      # Calculate the three statistics determined above except this time only \n",
        "      # compute them for the episodes within the batch just completed\n",
        "      batch_avg_reward = sum(total_reward[(episode + 1 - batch_size):(episode + 1)])/batch_size\n",
        "      batch_avg_episode_length = sum(total_episode_length[(episode + 1 - batch_size):(episode + 1)])/batch_size\n",
        "      episode_result_batch = sum(total_episode_result[(episode + 1 - batch_size):(episode + 1)])\n",
        "      average_episode_error_batch = sum(average_episode_error_list[(episode + 1 - batch_size):(episode + 1)])/batch_size\n",
        "\n",
        "      if batch_printout == True:\n",
        "        # Display current learning rate 𝛼 and and 𝜀 for 𝜀-greedy policy\n",
        "        print(' Current Learning Rate and Exploration Factor')\n",
        "        print('   Current Learning Rate - alpha (𝛼): ', optimizer.lr.numpy())\n",
        "        print('   Current Exploration Factor - epsilon (𝜀): ', eps, '\\n')\n",
        "\n",
        "        # Display the results of the statistics calculated above - first the \n",
        "        # cumulative statistics, then the results for the batch\n",
        "        print(' Cumulative Statistics:')\n",
        "\n",
        "        # Total average reward attained\n",
        "        print('   Cumulative Average Reward = ', total_avg_reward)\n",
        "\n",
        "        # Average episode length along with its fractional length (out of max\n",
        "        # possible episode length)\n",
        "        print('   Cumulative Average Episode Length = ', total_avg_episode_length, '/', max_episode_steps, ' (', total_avg_episode_length/max_episode_steps, ')')     \n",
        "        \n",
        "        # Number of successful episodes (out of all episodes)\n",
        "        print('   Cumulative Successful Episode Count = ', episode_result_cumulative, '/', len(total_episode_result), ' (',  episode_result_cumulative/len(total_episode_result), ')')     \n",
        "\n",
        "        # Average per prediction error between function approximation and 'actual' value function\n",
        "        print('   Cumulative Average of Average Per Prediction Episode Error = ', average_episode_error_cumulative, ' (Avg. over', len(average_episode_error_list), 'episodes)')     \n",
        "\n",
        "        # Similar description of statistics, but this time just pertaining to the\n",
        "        # batch\n",
        "        print('\\n Batch Statistics (episodes ', (episode + 1) - batch_size + 1, ' to ', episode + 1, '):')\n",
        "        print('   Batch Average Reward = ', batch_avg_reward)\n",
        "        print('   Batch Average Episode Length = ', batch_avg_episode_length, '/', max_episode_steps, ' (', batch_avg_episode_length/max_episode_steps, ')')           \n",
        "        print('   Batch Successful Episode Count = ', episode_result_batch, '/', batch_size, ' (', episode_result_batch/batch_size, ')') \n",
        "        print('   Batch Average of Average Per Prediction Episode Error = ', average_episode_error_batch, ' (Avg. over', batch_size, 'episodes)\\n')\n",
        "    # If all episodes in the episode_count have been run, training is complete\n",
        "    # and method, selected parameters, and final results will be displayed\n",
        "    if episode + 1 == episode_count:\n",
        "\n",
        "      print('-' * 81)\n",
        "      print('𝐓𝐑𝐀𝐈𝐍𝐈𝐍𝐆 𝐂𝐎𝐌𝐏𝐋𝐄𝐓𝐄')\n",
        "      \n",
        "      # Determine how to display chosen TD control method based on user \n",
        "      # inputs to the relevant parameters ('n' and 'method') \n",
        "      selected_method = 'n-SARSA (n = ' + str(n) + ')'\n",
        "\n",
        "      # Print the selected method\n",
        "      print('ᴛᴅ-ᴄᴏɴᴛʀᴏʟ ᴍᴇᴛʜᴏᴅ:')\n",
        "      print(' ', selected_method)\n",
        "\n",
        "      # Print the training hyperparameters used\n",
        "      print('\\nᴘᴀʀᴀᴍᴇᴛᴇʀꜱ:')\n",
        "      print(' Final Learning rate (𝜶) = ', optimizer.lr.numpy()) \n",
        "      print(' Discount Factor (𝜸) = ', gamma)\n",
        "      print(' Final 𝜀-greedy constant (𝜺) = ', eps)\n",
        "      \n",
        "      # Print cumulative statistics after all episodes have run\n",
        "      print('\\nꜱᴜᴍᴍᴀʀʏ ꜱᴛᴀᴛɪꜱᴛɪᴄꜱ:')\n",
        "      \n",
        "      # Create a list with the episode lengths for the successful episodes only\n",
        "      successful_episodes = [total_episode_length[x] for x in range(0, len(total_episode_length)) if total_episode_result[x] == 1]\n",
        "      \n",
        "      # Calculate the average of episode length for successful episodes only\n",
        "      avg_successful_episode_length = sum(successful_episodes)/len(successful_episodes) if len(successful_episodes) !=0 else 0\n",
        "      \n",
        "      # Display average episode length including failed episodes\n",
        "      print(' Cumulative Average Episode Length = ', total_avg_episode_length, '/', max_episode_steps, ' (', total_avg_episode_length/max_episode_steps, ')')\n",
        "      \n",
        "      # Display average episode length excluding failed episodes\n",
        "      print(' Cumulative Average Successful Episode Length: ', avg_successful_episode_length, '/', max_episode_steps, ' (', avg_successful_episode_length/max_episode_steps, ')')\n",
        "      \n",
        "      # Display the count of successful episodes \n",
        "      print(' Cumulative Successful Episode Count = ', episode_result_cumulative, '/', len(total_episode_result), ' (',  episode_result_cumulative/len(total_episode_result), ')')     \n",
        "\n",
        "      # Display cumulative average of average errors between NN based action\n",
        "      # value function and estimation of action value function\n",
        "      print(' Cumulative Average of Average Per Prediction Episode Error = ', average_episode_error_cumulative, ' (Avg. over', len(average_episode_error_list), 'episodes)')    \n",
        "\n",
        "  # Return the final policy and action value function after all episodes in the\n",
        "  # episode count have run\n",
        "  return model"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ev8ZQ2z4AkW"
      },
      "source": [
        "##**5. Performance Assessment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2i0IRqs466s"
      },
      "source": [
        "###CartPole Performance Simulation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4juaJcx4GQj"
      },
      "source": [
        "# Function to demonstrate performance of agent\n",
        "def CartPole_TEST(model, trial_count, episode_count, eps = 0.1, display_trial_readout = True, display_episode_readout = True):\n",
        "\n",
        "  # QUICK DEFINITION: trial - a group of episodes\n",
        "  # 'episode_count': this parameter represents how many episodes are in each \n",
        "  # trial\n",
        "\n",
        "  # Episodes can only last 200 timesteps - do not change this variable\n",
        "  max_episode_length = 200\n",
        "\n",
        "  # Length of timesteps for each trial (trial is a set of episodes)\n",
        "  trial_step_count = episode_count * max_episode_length\n",
        "\n",
        "  # Generate the MC environment\n",
        "  env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "  # Initialize lists which will store \n",
        "  successful_episode_average_list = []\n",
        "  successful_episode_count_list = []\n",
        "  episode_average_list = []\n",
        "  # Loop for each trial\n",
        "  for trial in range(0, trial_count):\n",
        "    \n",
        "    # For each trial initialize a list into which the length of episodes will \n",
        "    # be stored\n",
        "    episode_steps = []\n",
        " \n",
        "    # Initialize a list which will indicate whether or not episodes have \n",
        "    # succeeded or failed\n",
        "    successful_episode_bool = []\n",
        "\n",
        "    # Loop for each episode within the trial\n",
        "    for episode in range(0, episode_count):\n",
        "      \n",
        "      # Reset the environment before processing an episode \n",
        "      env.reset()\n",
        "      for i in range(50000):\n",
        "\n",
        "        # Choose an action and move forward to the next time step\n",
        "        action = action_selection_NN(env, env.env.state, model, eps = eps)\n",
        "        obs, reward, done, info = env.step(action)\n",
        "\n",
        "        # If termination is reached, end the episode\n",
        "        if done:\n",
        "          break\n",
        "\n",
        "      # Determine if episode is successful\n",
        "      episode_result = episode_verdict(env.env.state)\n",
        "      successful_episode_bool.append(episode_result)\n",
        "\n",
        "      # Append the number of episode steps completed in an episode\n",
        "      episode_steps.append(i + 1)\n",
        "\n",
        "    # Create a list containing only the step counts for episodes that were \n",
        "    # successful \n",
        "    successful_episode_steps = [steps for success, steps in zip(successful_episode_bool, episode_steps) if success == 1]\n",
        "\n",
        "    # Determine the average step count of successful episodes\n",
        "    successful_episode_average = (sum(successful_episode_steps)/len(successful_episode_steps)) if len(successful_episode_steps) !=0 else 0\n",
        "    \n",
        "    # Determine the amount of successful episodes\n",
        "    successful_episode_count = len(successful_episode_steps)\n",
        "\n",
        "    # Determine the average length of all episodes\n",
        "    episode_average = sum(episode_steps)/len(episode_steps)\n",
        "\n",
        "    # Append the above calculations to a list that will store these measures\n",
        "    # in order to average them over all trials\n",
        "    successful_episode_average_list.append(successful_episode_average)\n",
        "    successful_episode_count_list.append(successful_episode_count)\n",
        "    episode_average_list.append(episode_average)\n",
        "\n",
        "\n",
        "    # Here we will display the output of our calculations - output can be \n",
        "    # seen on an trial-by-trial basis - or, more granularly, an episode-by-\n",
        "    # -episode basis. These can also both be turned off with the 'display_' + x\n",
        "    # + '_readout' parameters to this function - when they are turned off only\n",
        "    # the final cumulative statistics will be seen \n",
        "    if display_trial_readout == True or display_episode_readout == True:\n",
        "      print('*****Trial ', trial + 1, ' (',episode_count, 'Episodes) *****************')\n",
        "\n",
        "    if display_episode_readout == True: \n",
        "        \n",
        "      for episode in range(0, episode_count):\n",
        "\n",
        "        # Print whether episode has failed or succeeded\n",
        "        print('\\n Episode ', episode + 1, ':')\n",
        "\n",
        "        if successful_episode_bool[episode] == 0:\n",
        "          print('  FAILED EPISODE...')\n",
        "\n",
        "        else:\n",
        "          print('  SUCCESSFUL EPISODE...')\n",
        "\n",
        "        # Print the length of the episode\n",
        "        print('    Episode Length = ', episode_steps[episode], '/', max_episode_length)\n",
        "        print('    Fractional Length = ', episode_steps[episode]/max_episode_length)\n",
        "\n",
        "    if display_trial_readout == True:\n",
        "      # For each trial, display the number of successful episodes and \n",
        "      # average successful episode length; they are also displayed in fraction-\n",
        "      # al form with the print statements below\n",
        "      print('Successful Episodes = ', sum(successful_episode_bool), '/', episode_count)\n",
        "      print('Fraction of Successful Episodes = ', sum(successful_episode_bool)/episode_count)\n",
        "      print('Average Successful Episode Length = ', successful_episode_average, '/', max_episode_length)\n",
        "      print('Fraction Successful Episode Length = ', successful_episode_average/max_episode_length)\n",
        "      print('Average Episode Length = ', episode_average, '/', max_episode_length)\n",
        "      print('Fractional Average Episode Length = ', episode_average/max_episode_length, '\\n')\n",
        "\n",
        "  # Cumulative results from the trials are calculated here\n",
        "  total_successful_episode_count = sum(successful_episode_count_list)\n",
        "  total_successful_episode_average = sum(successful_episode_average_list)/len(successful_episode_average_list) if len(successful_episode_average_list) !=0 else 0\n",
        "  total_episode_average = sum(episode_average_list)/len(episode_average_list)\n",
        "\n",
        "  # Display cumulative statistics from the trial runs\n",
        "  print('CUMULATIVE RESULTS:')\n",
        "  print('Total Number of Successful Episodes = ', total_successful_episode_count, '/', (episode_count * trial_count))\n",
        "  print('Fraction of Successful Episodes = ', total_successful_episode_count/(episode_count * trial_count))\n",
        "  print('Average of Average Successful Episode Steps per Trial = ', total_successful_episode_average, '/', max_episode_length)\n",
        "  print('Average of Average Successful Episode Steps per Trial (Fractional) = ', total_successful_episode_average/max_episode_length)\n",
        "  print('Average of Average Successful Episode Steps per Trial = ', total_episode_average, '/', max_episode_length)\n",
        "  print('Average of Average Episode Lengths per Trial (Fractional) = ', total_episode_average/max_episode_length)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Dpl7yVDw7m6"
      },
      "source": [
        "#**V. Building the Neural Network for Function Approximation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Syytv0Xw2mDm"
      },
      "source": [
        "##**Model Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HZ2G8B3uw32"
      },
      "source": [
        "# Define dimension of input and output to the model\n",
        "p_inputSpace = NN_input(env)\n",
        "p_outputSpace = 1\n",
        "\n",
        "# Construct the model \n",
        "\n",
        "# Input Layer \n",
        "inputs = keras.Input(shape= (p_inputSpace,), name= 'state-action-pair')\n",
        "\n",
        "# First hidden layer - 256 neurons, tanh activation function\n",
        "x1 = keras.layers.Dense(256, activation= 'tanh', name = 'hidden_layer_1')(inputs)\n",
        "\n",
        "# Second hidden layer - 128 neurons, tanh activation function\n",
        "x2 = keras.layers.Dense(128, activation= 'tanh', name = 'hidden_layer_2')(x1)\n",
        "\n",
        "# Output layer\n",
        "outputs = keras.layers.Dense(units = p_outputSpace, name= 'action-value')(x2)\n",
        "\n",
        "# Assemble model after architecture has been specified\n",
        "model = keras.Model(inputs = inputs, outputs = outputs)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCYPNGMJ8wpf"
      },
      "source": [
        "##**Display Model Architecture Plot**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 152
        },
        "id": "ekSOZO7v8zD4",
        "outputId": "90add9ad-2e56-475d-f82d-a9755103e2b1"
      },
      "source": [
        "# plot a schematic of the model built in the previous code block\n",
        "keras.utils.plot_model(\n",
        "    \n",
        "    # Specify the model to plot\n",
        "    model = model,\n",
        "\n",
        "    # Display inputs and outputs dimensions to layers\n",
        "    show_shapes=True,\n",
        "\n",
        "    # Display type for inputs and outputs of layers\n",
        "    show_dtype=True,\n",
        "\n",
        "    # Display provided names of layers\n",
        "    show_layer_names=True,\n",
        "\n",
        "    # Orientation of model plot \n",
        "    rankdir=\"LR\", #TB: vertical; LR: hor\n",
        "    expand_nested=True,\n",
        "    \n",
        "    # Dots per inch - size of model plot \n",
        "    dpi= 96,\n",
        ")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miOvlQdi6mwS"
      },
      "source": [
        "##**Establish Optimizer Type and Error Metric**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBluUz2gz5Lh"
      },
      "source": [
        "# Optimizer and per-prediction error - SGD optimizer is being used in this case\n",
        "#optimizer = keras.optimizers.SGD(learning_rate = alpha)\n",
        "\n",
        "# Mean Squared Error will be used to determine the performance of the model\n",
        "# In the context of a Reinforcement Learning problem such as this, estimated\n",
        "# returns will be compared to the estimated action value generated by the \n",
        "# NN\n",
        "f_ppError = keras.losses.MeanSquaredError() "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9WgBHfY7Eb4"
      },
      "source": [
        "#**VI. n-SARSA With Function Approximation Implementation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcldbDZb7ygn"
      },
      "source": [
        "##**Initialized Inputs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0etH8adY_iv"
      },
      "source": [
        "model = initialize_model_weights(model, initialization = 'gaussian', mean = 0, std_dev = 0.7)\n",
        "weights_initial = model.get_weights()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQe50BaGbOXj"
      },
      "source": [
        "##**SARSA(0) Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hzi0IfKP7Dse",
        "outputId": "1bf11bb4-3b14-42c4-f011-9e268aa18dd6"
      },
      "source": [
        "# Reinitialize model weights to the values generate with the\n",
        "# 'initialize_model_weights' function\n",
        "model = reset_weights(model, weights_initial)\n",
        "\n",
        "alpha = 0.001\n",
        "\n",
        "# Optimizer and per-prediction error - SGD optimizer is being used in this case\n",
        "optimizer = keras.optimizers.SGD(learning_rate = alpha)\n",
        "\n",
        "# Mean Squared Error will be used to determine the performance of the model\n",
        "# In the context of a Reinforcement Learning problem such as this, estimated\n",
        "# returns will be compared to the estimated action value generated by the \n",
        "# NN\n",
        "f_ppError = keras.losses.MeanSquaredError() \n",
        "\n",
        "model_SARSA_0 = TD_CONTROL(env, model, f_ppError, optimizer = optimizer, n = 0, eps = 0.1, gamma = 0.92, episode_count = 1500, batch_printout = True)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100  episodes completed... out of  1500 ****************************************\n",
            " Current Learning Rate and Exploration Factor\n",
            "   Current Learning Rate - alpha (𝛼):  0.0005\n",
            "   Current Exploration Factor - epsilon (𝜀):  0.09801 \n",
            "\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  102.61\n",
            "   Cumulative Average Episode Length =  58.41 / 200  ( 0.29205 )\n",
            "   Cumulative Successful Episode Count =  1 / 100  ( 0.01 )\n",
            "   Cumulative Average of Average Per Prediction Episode Error =  9.709579006123057  (Avg. over 100 episodes)\n",
            "\n",
            " Batch Statistics (episodes  1  to  100 ):\n",
            "   Batch Average Reward =  102.61\n",
            "   Batch Average Episode Length =  58.41 / 200  ( 0.29205 )\n",
            "   Batch Successful Episode Count =  1 / 100  ( 0.01 )\n",
            "   Batch Average of Average Per Prediction Episode Error =  9.709579006123057  (Avg. over 100 episodes)\n",
            "\n",
            "200  episodes completed... out of  1500 ****************************************\n",
            " Current Learning Rate and Exploration Factor\n",
            "   Current Learning Rate - alpha (𝛼):  0.00033333336\n",
            "   Current Exploration Factor - epsilon (𝜀):  0.096059601 \n",
            "\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  134.705\n",
            "   Cumulative Average Episode Length =  78.36 / 200  ( 0.3918 )\n",
            "   Cumulative Successful Episode Count =  1 / 200  ( 0.005 )\n",
            "   Cumulative Average of Average Per Prediction Episode Error =  7.977215137989813  (Avg. over 200 episodes)\n",
            "\n",
            " Batch Statistics (episodes  101  to  200 ):\n",
            "   Batch Average Reward =  166.8\n",
            "   Batch Average Episode Length =  98.31 / 200  ( 0.49155 )\n",
            "   Batch Successful Episode Count =  0 / 100  ( 0.0 )\n",
            "   Batch Average of Average Per Prediction Episode Error =  6.244851269856571  (Avg. over 100 episodes)\n",
            "\n",
            "300  episodes completed... out of  1500 ****************************************\n",
            " Current Learning Rate and Exploration Factor\n",
            "   Current Learning Rate - alpha (𝛼):  0.0003\n",
            "   Current Exploration Factor - epsilon (𝜀):  0.0941480149401 \n",
            "\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  165.54333333333332\n",
            "   Cumulative Average Episode Length =  94.37 / 200  ( 0.47185000000000005 )\n",
            "   Cumulative Successful Episode Count =  4 / 300  ( 0.013333333333333334 )\n",
            "   Cumulative Average of Average Per Prediction Episode Error =  7.345114109756026  (Avg. over 300 episodes)\n",
            "\n",
            " Batch Statistics (episodes  201  to  300 ):\n",
            "   Batch Average Reward =  227.22\n",
            "   Batch Average Episode Length =  126.39 / 200  ( 0.63195 )\n",
            "   Batch Successful Episode Count =  3 / 100  ( 0.03 )\n",
            "   Batch Average of Average Per Prediction Episode Error =  6.080912053288455  (Avg. over 100 episodes)\n",
            "\n",
            "400  episodes completed... out of  1500 ****************************************\n",
            " Current Learning Rate and Exploration Factor\n",
            "   Current Learning Rate - alpha (𝛼):  0.0003\n",
            "   Current Exploration Factor - epsilon (𝜀):  0.09227446944279201 \n",
            "\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  191.3675\n",
            "   Cumulative Average Episode Length =  102.3325 / 200  ( 0.5116625 )\n",
            "   Cumulative Successful Episode Count =  15 / 400  ( 0.0375 )\n",
            "   Cumulative Average of Average Per Prediction Episode Error =  7.542604747538223  (Avg. over 400 episodes)\n",
            "\n",
            " Batch Statistics (episodes  301  to  400 ):\n",
            "   Batch Average Reward =  268.84\n",
            "   Batch Average Episode Length =  126.22 / 200  ( 0.6311 )\n",
            "   Batch Successful Episode Count =  11 / 100  ( 0.11 )\n",
            "   Batch Average of Average Per Prediction Episode Error =  8.135076660884828  (Avg. over 100 episodes)\n",
            "\n",
            "500  episodes completed... out of  1500 ****************************************\n",
            " Current Learning Rate and Exploration Factor\n",
            "   Current Learning Rate - alpha (𝛼):  0.0003\n",
            "   Current Exploration Factor - epsilon (𝜀):  0.09043820750088044 \n",
            "\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  209.48\n",
            "   Cumulative Average Episode Length =  108.692 / 200  ( 0.5434599999999999 )\n",
            "   Cumulative Successful Episode Count =  38 / 500  ( 0.076 )\n",
            "   Cumulative Average of Average Per Prediction Episode Error =  7.716754463518382  (Avg. over 500 episodes)\n",
            "\n",
            " Batch Statistics (episodes  401  to  500 ):\n",
            "   Batch Average Reward =  281.93\n",
            "   Batch Average Episode Length =  134.13 / 200  ( 0.67065 )\n",
            "   Batch Successful Episode Count =  23 / 100  ( 0.23 )\n",
            "   Batch Average of Average Per Prediction Episode Error =  8.41335332743904  (Avg. over 100 episodes)\n",
            "\n",
            "600  episodes completed... out of  1500 ****************************************\n",
            " Current Learning Rate and Exploration Factor\n",
            "   Current Learning Rate - alpha (𝛼):  0.0003\n",
            "   Current Exploration Factor - epsilon (𝜀):  0.08863848717161292 \n",
            "\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  230.54166666666666\n",
            "   Cumulative Average Episode Length =  116.73833333333333 / 200  ( 0.5836916666666666 )\n",
            "   Cumulative Successful Episode Count =  73 / 600  ( 0.12166666666666667 )\n",
            "   Cumulative Average of Average Per Prediction Episode Error =  7.936688307997105  (Avg. over 600 episodes)\n",
            "\n",
            " Batch Statistics (episodes  501  to  600 ):\n",
            "   Batch Average Reward =  335.85\n",
            "   Batch Average Episode Length =  156.97 / 200  ( 0.78485 )\n",
            "   Batch Successful Episode Count =  35 / 100  ( 0.35 )\n",
            "   Batch Average of Average Per Prediction Episode Error =  9.0363575303907  (Avg. over 100 episodes)\n",
            "\n",
            "700  episodes completed... out of  1500 ****************************************\n",
            " Current Learning Rate and Exploration Factor\n",
            "   Current Learning Rate - alpha (𝛼):  0.0003\n",
            "   Current Exploration Factor - epsilon (𝜀):  0.08687458127689782 \n",
            "\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  238.99\n",
            "   Cumulative Average Episode Length =  120.45 / 200  ( 0.6022500000000001 )\n",
            "   Cumulative Successful Episode Count =  97 / 700  ( 0.13857142857142857 )\n",
            "   Cumulative Average of Average Per Prediction Episode Error =  7.902787608679298  (Avg. over 700 episodes)\n",
            "\n",
            " Batch Statistics (episodes  601  to  700 ):\n",
            "   Batch Average Reward =  289.68\n",
            "   Batch Average Episode Length =  142.72 / 200  ( 0.7136 )\n",
            "   Batch Successful Episode Count =  24 / 100  ( 0.24 )\n",
            "   Batch Average of Average Per Prediction Episode Error =  7.699383412772423  (Avg. over 100 episodes)\n",
            "\n",
            "800  episodes completed... out of  1500 ****************************************\n",
            " Current Learning Rate and Exploration Factor\n",
            "   Current Learning Rate - alpha (𝛼):  0.0003\n",
            "   Current Exploration Factor - epsilon (𝜀):  0.08514577710948755 \n",
            "\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  241.96\n",
            "   Cumulative Average Episode Length =  122.58875 / 200  ( 0.61294375 )\n",
            "   Cumulative Successful Episode Count =  115 / 800  ( 0.14375 )\n",
            "   Cumulative Average of Average Per Prediction Episode Error =  7.894095158669577  (Avg. over 800 episodes)\n",
            "\n",
            " Batch Statistics (episodes  701  to  800 ):\n",
            "   Batch Average Reward =  262.75\n",
            "   Batch Average Episode Length =  137.56 / 200  ( 0.6878 )\n",
            "   Batch Successful Episode Count =  18 / 100  ( 0.18 )\n",
            "   Batch Average of Average Per Prediction Episode Error =  7.833248008601536  (Avg. over 100 episodes)\n",
            "\n",
            "900  episodes completed... out of  1500 ****************************************\n",
            " Current Learning Rate and Exploration Factor\n",
            "   Current Learning Rate - alpha (𝛼):  0.0003\n",
            "   Current Exploration Factor - epsilon (𝜀):  0.08345137614500873 \n",
            "\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  244.89111111111112\n",
            "   Cumulative Average Episode Length =  124.43 / 200  ( 0.62215 )\n",
            "   Cumulative Successful Episode Count =  131 / 900  ( 0.14555555555555555 )\n",
            "   Cumulative Average of Average Per Prediction Episode Error =  7.809101666805478  (Avg. over 900 episodes)\n",
            "\n",
            " Batch Statistics (episodes  801  to  900 ):\n",
            "   Batch Average Reward =  268.34\n",
            "   Batch Average Episode Length =  139.16 / 200  ( 0.6958 )\n",
            "   Batch Successful Episode Count =  16 / 100  ( 0.16 )\n",
            "   Batch Average of Average Per Prediction Episode Error =  7.129153731892677  (Avg. over 100 episodes)\n",
            "\n",
            "1000  episodes completed... out of  1500 ****************************************\n",
            " Current Learning Rate and Exploration Factor\n",
            "   Current Learning Rate - alpha (𝛼):  0.0003\n",
            "   Current Exploration Factor - epsilon (𝜀):  0.08179069375972306 \n",
            "\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  245.095\n",
            "   Cumulative Average Episode Length =  125.806 / 200  ( 0.62903 )\n",
            "   Cumulative Successful Episode Count =  136 / 1000  ( 0.136 )\n",
            "   Cumulative Average of Average Per Prediction Episode Error =  7.652320949566288  (Avg. over 1000 episodes)\n",
            "\n",
            " Batch Statistics (episodes  901  to  1000 ):\n",
            "   Batch Average Reward =  246.93\n",
            "   Batch Average Episode Length =  138.19 / 200  ( 0.69095 )\n",
            "   Batch Successful Episode Count =  5 / 100  ( 0.05 )\n",
            "   Batch Average of Average Per Prediction Episode Error =  6.241294494413628  (Avg. over 100 episodes)\n",
            "\n",
            "1100  episodes completed... out of  1500 ****************************************\n",
            " Current Learning Rate and Exploration Factor\n",
            "   Current Learning Rate - alpha (𝛼):  0.0003\n",
            "   Current Exploration Factor - epsilon (𝜀):  0.08016305895390458 \n",
            "\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  246.27\n",
            "   Cumulative Average Episode Length =  127.19545454545455 / 200  ( 0.6359772727272728 )\n",
            "   Cumulative Successful Episode Count =  144 / 1100  ( 0.13090909090909092 )\n",
            "   Cumulative Average of Average Per Prediction Episode Error =  7.510640243275265  (Avg. over 1100 episodes)\n",
            "\n",
            " Batch Statistics (episodes  1001  to  1100 ):\n",
            "   Batch Average Reward =  258.02\n",
            "   Batch Average Episode Length =  141.09 / 200  ( 0.70545 )\n",
            "   Batch Successful Episode Count =  8 / 100  ( 0.08 )\n",
            "   Batch Average of Average Per Prediction Episode Error =  6.093833180365064  (Avg. over 100 episodes)\n",
            "\n",
            "1200  episodes completed... out of  1500 ****************************************\n",
            " Current Learning Rate and Exploration Factor\n",
            "   Current Learning Rate - alpha (𝛼):  0.0003\n",
            "   Current Exploration Factor - epsilon (𝜀):  0.07856781408072187 \n",
            "\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  250.92416666666668\n",
            "   Cumulative Average Episode Length =  129.47333333333333 / 200  ( 0.6473666666666666 )\n",
            "   Cumulative Successful Episode Count =  177 / 1200  ( 0.1475 )\n",
            "   Cumulative Average of Average Per Prediction Episode Error =  7.510457012642987  (Avg. over 1200 episodes)\n",
            "\n",
            " Batch Statistics (episodes  1101  to  1200 ):\n",
            "   Batch Average Reward =  302.12\n",
            "   Batch Average Episode Length =  154.53 / 200  ( 0.7726500000000001 )\n",
            "   Batch Successful Episode Count =  33 / 100  ( 0.33 )\n",
            "   Batch Average of Average Per Prediction Episode Error =  7.508441475687889  (Avg. over 100 episodes)\n",
            "\n",
            "1300  episodes completed... out of  1500 ****************************************\n",
            " Current Learning Rate and Exploration Factor\n",
            "   Current Learning Rate - alpha (𝛼):  0.0003\n",
            "   Current Exploration Factor - epsilon (𝜀):  0.0770043145805155 \n",
            "\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  256.4546153846154\n",
            "   Cumulative Average Episode Length =  131.69 / 200  ( 0.65845 )\n",
            "   Cumulative Successful Episode Count =  201 / 1300  ( 0.15461538461538463 )\n",
            "   Cumulative Average of Average Per Prediction Episode Error =  7.511870213380452  (Avg. over 1300 episodes)\n",
            "\n",
            " Batch Statistics (episodes  1201  to  1300 ):\n",
            "   Batch Average Reward =  322.82\n",
            "   Batch Average Episode Length =  158.29 / 200  ( 0.79145 )\n",
            "   Batch Successful Episode Count =  24 / 100  ( 0.24 )\n",
            "   Batch Average of Average Per Prediction Episode Error =  7.528828622230093  (Avg. over 100 episodes)\n",
            "\n",
            "1400  episodes completed... out of  1500 ****************************************\n",
            " Current Learning Rate and Exploration Factor\n",
            "   Current Learning Rate - alpha (𝛼):  0.0003\n",
            "   Current Exploration Factor - epsilon (𝜀):  0.07547192872036323 \n",
            "\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  264.795\n",
            "   Cumulative Average Episode Length =  134.18 / 200  ( 0.6709 )\n",
            "   Cumulative Successful Episode Count =  245 / 1400  ( 0.175 )\n",
            "   Cumulative Average of Average Per Prediction Episode Error =  7.625714987359635  (Avg. over 1400 episodes)\n",
            "\n",
            " Batch Statistics (episodes  1301  to  1400 ):\n",
            "   Batch Average Reward =  373.22\n",
            "   Batch Average Episode Length =  166.55 / 200  ( 0.8327500000000001 )\n",
            "   Batch Successful Episode Count =  44 / 100  ( 0.44 )\n",
            "   Batch Average of Average Per Prediction Episode Error =  9.105697049088947  (Avg. over 100 episodes)\n",
            "\n",
            "1500  episodes completed... out of  1500 ****************************************\n",
            " Current Learning Rate and Exploration Factor\n",
            "   Current Learning Rate - alpha (𝛼):  0.0003\n",
            "   Current Exploration Factor - epsilon (𝜀):  0.073970037338828 \n",
            "\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  273.09866666666665\n",
            "   Cumulative Average Episode Length =  137.06333333333333 / 200  ( 0.6853166666666667 )\n",
            "   Cumulative Successful Episode Count =  301 / 1500  ( 0.20066666666666666 )\n",
            "   Cumulative Average of Average Per Prediction Episode Error =  7.698886132285198  (Avg. over 1500 episodes)\n",
            "\n",
            " Batch Statistics (episodes  1401  to  1500 ):\n",
            "   Batch Average Reward =  389.35\n",
            "   Batch Average Episode Length =  177.43 / 200  ( 0.88715 )\n",
            "   Batch Successful Episode Count =  56 / 100  ( 0.56 )\n",
            "   Batch Average of Average Per Prediction Episode Error =  8.723282161243041  (Avg. over 100 episodes)\n",
            "\n",
            "---------------------------------------------------------------------------------\n",
            "𝐓𝐑𝐀𝐈𝐍𝐈𝐍𝐆 𝐂𝐎𝐌𝐏𝐋𝐄𝐓𝐄\n",
            "ᴛᴅ-ᴄᴏɴᴛʀᴏʟ ᴍᴇᴛʜᴏᴅ:\n",
            "  n-SARSA (n = 0)\n",
            "\n",
            "ᴘᴀʀᴀᴍᴇᴛᴇʀꜱ:\n",
            " Final Learning rate (𝜶) =  0.0003\n",
            " Discount Factor (𝜸) =  0.92\n",
            " Final 𝜀-greedy constant (𝜺) =  0.073970037338828\n",
            "\n",
            "ꜱᴜᴍᴍᴀʀʏ ꜱᴛᴀᴛɪꜱᴛɪᴄꜱ:\n",
            " Cumulative Average Episode Length =  137.06333333333333 / 200  ( 0.6853166666666667 )\n",
            " Cumulative Average Successful Episode Length:  200.0 / 200  ( 1.0 )\n",
            " Cumulative Successful Episode Count =  301 / 1500  ( 0.20066666666666666 )\n",
            " Cumulative Average of Average Per Prediction Episode Error =  7.698886132285198  (Avg. over 1500 episodes)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE98eN9y5Khv"
      },
      "source": [
        "##**SARSA(0) Performance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULxqNxCeflPu",
        "outputId": "a263ec2e-d9f8-4670-92df-31781d39efff"
      },
      "source": [
        "CartPole_TEST(model_SARSA_0, trial_count = 10, episode_count = 100, eps = 0.1, display_trial_readout = True, display_episode_readout = False)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*****Trial  1  ( 100 Episodes) *****************\n",
            "Successful Episodes =  100 / 100\n",
            "Fraction of Successful Episodes =  1.0\n",
            "Average Successful Episode Length =  200.0 / 200\n",
            "Fraction Successful Episode Length =  1.0\n",
            "Average Episode Length =  200.0 / 200\n",
            "Fractional Average Episode Length =  1.0 \n",
            "\n",
            "*****Trial  2  ( 100 Episodes) *****************\n",
            "Successful Episodes =  98 / 100\n",
            "Fraction of Successful Episodes =  0.98\n",
            "Average Successful Episode Length =  200.0 / 200\n",
            "Fraction Successful Episode Length =  1.0\n",
            "Average Episode Length =  199.82 / 200\n",
            "Fractional Average Episode Length =  0.9991 \n",
            "\n",
            "*****Trial  3  ( 100 Episodes) *****************\n",
            "Successful Episodes =  99 / 100\n",
            "Fraction of Successful Episodes =  0.99\n",
            "Average Successful Episode Length =  200.0 / 200\n",
            "Fraction Successful Episode Length =  1.0\n",
            "Average Episode Length =  198.53 / 200\n",
            "Fractional Average Episode Length =  0.99265 \n",
            "\n",
            "*****Trial  4  ( 100 Episodes) *****************\n",
            "Successful Episodes =  100 / 100\n",
            "Fraction of Successful Episodes =  1.0\n",
            "Average Successful Episode Length =  200.0 / 200\n",
            "Fraction Successful Episode Length =  1.0\n",
            "Average Episode Length =  200.0 / 200\n",
            "Fractional Average Episode Length =  1.0 \n",
            "\n",
            "*****Trial  5  ( 100 Episodes) *****************\n",
            "Successful Episodes =  99 / 100\n",
            "Fraction of Successful Episodes =  0.99\n",
            "Average Successful Episode Length =  200.0 / 200\n",
            "Fraction Successful Episode Length =  1.0\n",
            "Average Episode Length =  199.28 / 200\n",
            "Fractional Average Episode Length =  0.9964 \n",
            "\n",
            "*****Trial  6  ( 100 Episodes) *****************\n",
            "Successful Episodes =  100 / 100\n",
            "Fraction of Successful Episodes =  1.0\n",
            "Average Successful Episode Length =  200.0 / 200\n",
            "Fraction Successful Episode Length =  1.0\n",
            "Average Episode Length =  200.0 / 200\n",
            "Fractional Average Episode Length =  1.0 \n",
            "\n",
            "*****Trial  7  ( 100 Episodes) *****************\n",
            "Successful Episodes =  100 / 100\n",
            "Fraction of Successful Episodes =  1.0\n",
            "Average Successful Episode Length =  200.0 / 200\n",
            "Fraction Successful Episode Length =  1.0\n",
            "Average Episode Length =  200.0 / 200\n",
            "Fractional Average Episode Length =  1.0 \n",
            "\n",
            "*****Trial  8  ( 100 Episodes) *****************\n",
            "Successful Episodes =  99 / 100\n",
            "Fraction of Successful Episodes =  0.99\n",
            "Average Successful Episode Length =  200.0 / 200\n",
            "Fraction Successful Episode Length =  1.0\n",
            "Average Episode Length =  199.41 / 200\n",
            "Fractional Average Episode Length =  0.99705 \n",
            "\n",
            "*****Trial  9  ( 100 Episodes) *****************\n",
            "Successful Episodes =  99 / 100\n",
            "Fraction of Successful Episodes =  0.99\n",
            "Average Successful Episode Length =  200.0 / 200\n",
            "Fraction Successful Episode Length =  1.0\n",
            "Average Episode Length =  199.61 / 200\n",
            "Fractional Average Episode Length =  0.9980500000000001 \n",
            "\n",
            "*****Trial  10  ( 100 Episodes) *****************\n",
            "Successful Episodes =  99 / 100\n",
            "Fraction of Successful Episodes =  0.99\n",
            "Average Successful Episode Length =  200.0 / 200\n",
            "Fraction Successful Episode Length =  1.0\n",
            "Average Episode Length =  199.4 / 200\n",
            "Fractional Average Episode Length =  0.997 \n",
            "\n",
            "CUMULATIVE RESULTS:\n",
            "Total Number of Successful Episodes =  993 / 1000\n",
            "Fraction of Successful Episodes =  0.993\n",
            "Average of Average Successful Episode Steps per Trial =  200.0 / 200\n",
            "Average of Average Successful Episode Steps per Trial (Fractional) =  1.0\n",
            "Average of Average Successful Episode Steps per Trial =  199.60500000000002 / 200\n",
            "Average of Average Episode Lengths per Trial (Fractional) =  0.998025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11Y-GeO35MwC"
      },
      "source": [
        "##**SARSA(n) Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YieZhZi5bCX4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84dd211f-deff-40dc-8f19-085193d68211"
      },
      "source": [
        "# Note hyperparameter tuning not done for SARSA(n) - but can be tested out\n",
        "# if you like\n",
        "\n",
        "# Reinitialize model weights\n",
        "model = reset_weights(model, weights_initial)\n",
        "\n",
        "# Reset learning rate\n",
        "alpha = 0.0009\n",
        "\n",
        "# Optimizer and per-prediction error - SGD optimizer is being used in this case\n",
        "optimizer = keras.optimizers.SGD(learning_rate = alpha)\n",
        "\n",
        "model_SARSA_N = TD_CONTROL(env, model, f_ppError, optimizer = optimizer, n = 2, eps = 0.1, gamma = 0.94, episode_count = 1500, batch_printout = True)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100  episodes completed... out of  1500 ****************************************\n",
            " Current Learning Rate and Exploration Factor\n",
            "   Current Learning Rate - alpha (𝛼):  0.00045\n",
            "   Current Exploration Factor - epsilon (𝜀):  0.09801 \n",
            "\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  98.91\n",
            "   Cumulative Average Episode Length =  56.53 / 200  ( 0.28265 )\n",
            "   Cumulative Successful Episode Count =  0 / 100  ( 0.0 )\n",
            "   Cumulative Average of Average Per Prediction Episode Error =  30.32558759806288  (Avg. over 100 episodes)\n",
            "\n",
            " Batch Statistics (episodes  1  to  100 ):\n",
            "   Batch Average Reward =  98.91\n",
            "   Batch Average Episode Length =  56.53 / 200  ( 0.28265 )\n",
            "   Batch Successful Episode Count =  0 / 100  ( 0.0 )\n",
            "   Batch Average of Average Per Prediction Episode Error =  30.32558759806288  (Avg. over 100 episodes)\n",
            "\n",
            "200  episodes completed... out of  1500 ****************************************\n",
            " Current Learning Rate and Exploration Factor\n",
            "   Current Learning Rate - alpha (𝛼):  0.0003\n",
            "   Current Exploration Factor - epsilon (𝜀):  0.096059601 \n",
            "\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  136.525\n",
            "   Cumulative Average Episode Length =  67.93 / 200  ( 0.33965 )\n",
            "   Cumulative Successful Episode Count =  1 / 200  ( 0.005 )\n",
            "   Cumulative Average of Average Per Prediction Episode Error =  33.4160427008444  (Avg. over 200 episodes)\n",
            "\n",
            " Batch Statistics (episodes  101  to  200 ):\n",
            "   Batch Average Reward =  174.14\n",
            "   Batch Average Episode Length =  79.33 / 200  ( 0.39665 )\n",
            "   Batch Successful Episode Count =  1 / 100  ( 0.01 )\n",
            "   Batch Average of Average Per Prediction Episode Error =  36.50649780362594  (Avg. over 100 episodes)\n",
            "\n",
            "300  episodes completed... out of  1500 ****************************************\n",
            " Current Learning Rate and Exploration Factor\n",
            "   Current Learning Rate - alpha (𝛼):  0.0003\n",
            "   Current Exploration Factor - epsilon (𝜀):  0.0941480149401 \n",
            "\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  167.65333333333334\n",
            "   Cumulative Average Episode Length =  78.28 / 200  ( 0.3914 )\n",
            "   Cumulative Successful Episode Count =  10 / 300  ( 0.03333333333333333 )\n",
            "   Cumulative Average of Average Per Prediction Episode Error =  36.72753475463545  (Avg. over 300 episodes)\n",
            "\n",
            " Batch Statistics (episodes  201  to  300 ):\n",
            "   Batch Average Reward =  229.91\n",
            "   Batch Average Episode Length =  98.98 / 200  ( 0.4949 )\n",
            "   Batch Successful Episode Count =  9 / 100  ( 0.09 )\n",
            "   Batch Average of Average Per Prediction Episode Error =  43.35051886221757  (Avg. over 100 episodes)\n",
            "\n",
            "400  episodes completed... out of  1500 ****************************************\n",
            " Current Learning Rate and Exploration Factor\n",
            "   Current Learning Rate - alpha (𝛼):  0.0003\n",
            "   Current Exploration Factor - epsilon (𝜀):  0.09227446944279201 \n",
            "\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  168.7775\n",
            "   Cumulative Average Episode Length =  79.5325 / 200  ( 0.3976625 )\n",
            "   Cumulative Successful Episode Count =  17 / 400  ( 0.0425 )\n",
            "   Cumulative Average of Average Per Prediction Episode Error =  37.58140347576455  (Avg. over 400 episodes)\n",
            "\n",
            " Batch Statistics (episodes  301  to  400 ):\n",
            "   Batch Average Reward =  172.15\n",
            "   Batch Average Episode Length =  83.29 / 200  ( 0.41645000000000004 )\n",
            "   Batch Successful Episode Count =  7 / 100  ( 0.07 )\n",
            "   Batch Average of Average Per Prediction Episode Error =  40.143009639151934  (Avg. over 100 episodes)\n",
            "\n",
            "500  episodes completed... out of  1500 ****************************************\n",
            " Current Learning Rate and Exploration Factor\n",
            "   Current Learning Rate - alpha (𝛼):  0.0003\n",
            "   Current Exploration Factor - epsilon (𝜀):  0.09043820750088044 \n",
            "\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  171.618\n",
            "   Cumulative Average Episode Length =  81.008 / 200  ( 0.40503999999999996 )\n",
            "   Cumulative Successful Episode Count =  22 / 500  ( 0.044 )\n",
            "   Cumulative Average of Average Per Prediction Episode Error =  38.46830903763126  (Avg. over 500 episodes)\n",
            "\n",
            " Batch Statistics (episodes  401  to  500 ):\n",
            "   Batch Average Reward =  182.98\n",
            "   Batch Average Episode Length =  86.91 / 200  ( 0.43455 )\n",
            "   Batch Successful Episode Count =  5 / 100  ( 0.05 )\n",
            "   Batch Average of Average Per Prediction Episode Error =  42.01593128509815  (Avg. over 100 episodes)\n",
            "\n",
            "600  episodes completed... out of  1500 ****************************************\n",
            " Current Learning Rate and Exploration Factor\n",
            "   Current Learning Rate - alpha (𝛼):  0.0003\n",
            "   Current Exploration Factor - epsilon (𝜀):  0.08863848717161292 \n",
            "\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  184.78166666666667\n",
            "   Cumulative Average Episode Length =  86.82166666666667 / 200  ( 0.4341083333333334 )\n",
            "   Cumulative Successful Episode Count =  43 / 600  ( 0.07166666666666667 )\n",
            "   Cumulative Average of Average Per Prediction Episode Error =  39.07210719995156  (Avg. over 600 episodes)\n",
            "\n",
            " Batch Statistics (episodes  501  to  600 ):\n",
            "   Batch Average Reward =  250.6\n",
            "   Batch Average Episode Length =  115.89 / 200  ( 0.57945 )\n",
            "   Batch Successful Episode Count =  21 / 100  ( 0.21 )\n",
            "   Batch Average of Average Per Prediction Episode Error =  42.09109801155306  (Avg. over 100 episodes)\n",
            "\n",
            "700  episodes completed... out of  1500 ****************************************\n",
            " Current Learning Rate and Exploration Factor\n",
            "   Current Learning Rate - alpha (𝛼):  0.0003\n",
            "   Current Exploration Factor - epsilon (𝜀):  0.08687458127689782 \n",
            "\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  194.81428571428572\n",
            "   Cumulative Average Episode Length =  91.19428571428571 / 200  ( 0.4559714285714286 )\n",
            "   Cumulative Successful Episode Count =  54 / 700  ( 0.07714285714285714 )\n",
            "   Cumulative Average of Average Per Prediction Episode Error =  39.70731267995045  (Avg. over 700 episodes)\n",
            "\n",
            " Batch Statistics (episodes  601  to  700 ):\n",
            "   Batch Average Reward =  255.01\n",
            "   Batch Average Episode Length =  117.43 / 200  ( 0.5871500000000001 )\n",
            "   Batch Successful Episode Count =  11 / 100  ( 0.11 )\n",
            "   Batch Average of Average Per Prediction Episode Error =  43.518545559943966  (Avg. over 100 episodes)\n",
            "\n",
            "800  episodes completed... out of  1500 ****************************************\n",
            " Current Learning Rate and Exploration Factor\n",
            "   Current Learning Rate - alpha (𝛼):  0.0003\n",
            "   Current Exploration Factor - epsilon (𝜀):  0.08514577710948755 \n",
            "\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  203.025\n",
            "   Cumulative Average Episode Length =  93.27875 / 200  ( 0.46639375 )\n",
            "   Cumulative Successful Episode Count =  73 / 800  ( 0.09125 )\n",
            "   Cumulative Average of Average Per Prediction Episode Error =  41.007918859234024  (Avg. over 800 episodes)\n",
            "\n",
            " Batch Statistics (episodes  701  to  800 ):\n",
            "   Batch Average Reward =  260.5\n",
            "   Batch Average Episode Length =  107.87 / 200  ( 0.53935 )\n",
            "   Batch Successful Episode Count =  19 / 100  ( 0.19 )\n",
            "   Batch Average of Average Per Prediction Episode Error =  50.11216211421908  (Avg. over 100 episodes)\n",
            "\n",
            "900  episodes completed... out of  1500 ****************************************\n",
            " Current Learning Rate and Exploration Factor\n",
            "   Current Learning Rate - alpha (𝛼):  0.0003\n",
            "   Current Exploration Factor - epsilon (𝜀):  0.08345137614500873 \n",
            "\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  204.77444444444444\n",
            "   Cumulative Average Episode Length =  92.99333333333334 / 200  ( 0.4649666666666667 )\n",
            "   Cumulative Successful Episode Count =  83 / 900  ( 0.09222222222222222 )\n",
            "   Cumulative Average of Average Per Prediction Episode Error =  41.86433928276734  (Avg. over 900 episodes)\n",
            "\n",
            " Batch Statistics (episodes  801  to  900 ):\n",
            "   Batch Average Reward =  218.77\n",
            "   Batch Average Episode Length =  90.71 / 200  ( 0.45354999999999995 )\n",
            "   Batch Successful Episode Count =  10 / 100  ( 0.1 )\n",
            "   Batch Average of Average Per Prediction Episode Error =  48.715702671034  (Avg. over 100 episodes)\n",
            "\n",
            "1000  episodes completed... out of  1500 ****************************************\n",
            " Current Learning Rate and Exploration Factor\n",
            "   Current Learning Rate - alpha (𝛼):  0.0003\n",
            "   Current Exploration Factor - epsilon (𝜀):  0.08179069375972306 \n",
            "\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  199.157\n",
            "   Cumulative Average Episode Length =  90.06 / 200  ( 0.45030000000000003 )\n",
            "   Cumulative Successful Episode Count =  91 / 1000  ( 0.091 )\n",
            "   Cumulative Average of Average Per Prediction Episode Error =  42.19805684556661  (Avg. over 1000 episodes)\n",
            "\n",
            " Batch Statistics (episodes  901  to  1000 ):\n",
            "   Batch Average Reward =  148.6\n",
            "   Batch Average Episode Length =  63.66 / 200  ( 0.31829999999999997 )\n",
            "   Batch Successful Episode Count =  8 / 100  ( 0.08 )\n",
            "   Batch Average of Average Per Prediction Episode Error =  45.20151491076021  (Avg. over 100 episodes)\n",
            "\n",
            "1100  episodes completed... out of  1500 ****************************************\n",
            " Current Learning Rate and Exploration Factor\n",
            "   Current Learning Rate - alpha (𝛼):  0.0003\n",
            "   Current Exploration Factor - epsilon (𝜀):  0.08016305895390458 \n",
            "\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  205.57818181818183\n",
            "   Cumulative Average Episode Length =  92.29454545454546 / 200  ( 0.46147272727272726 )\n",
            "   Cumulative Successful Episode Count =  112 / 1100  ( 0.10181818181818182 )\n",
            "   Cumulative Average of Average Per Prediction Episode Error =  43.04887369547707  (Avg. over 1100 episodes)\n",
            "\n",
            " Batch Statistics (episodes  1001  to  1100 ):\n",
            "   Batch Average Reward =  269.79\n",
            "   Batch Average Episode Length =  114.64 / 200  ( 0.5732 )\n",
            "   Batch Successful Episode Count =  21 / 100  ( 0.21 )\n",
            "   Batch Average of Average Per Prediction Episode Error =  51.55704219458201  (Avg. over 100 episodes)\n",
            "\n",
            "1200  episodes completed... out of  1500 ****************************************\n",
            " Current Learning Rate and Exploration Factor\n",
            "   Current Learning Rate - alpha (𝛼):  0.0003\n",
            "   Current Exploration Factor - epsilon (𝜀):  0.07856781408072187 \n",
            "\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  206.26\n",
            "   Cumulative Average Episode Length =  91.93166666666667 / 200  ( 0.45965833333333334 )\n",
            "   Cumulative Successful Episode Count =  128 / 1200  ( 0.10666666666666667 )\n",
            "   Cumulative Average of Average Per Prediction Episode Error =  43.567541465684165  (Avg. over 1200 episodes)\n",
            "\n",
            " Batch Statistics (episodes  1101  to  1200 ):\n",
            "   Batch Average Reward =  213.76\n",
            "   Batch Average Episode Length =  87.94 / 200  ( 0.4397 )\n",
            "   Batch Successful Episode Count =  16 / 100  ( 0.16 )\n",
            "   Batch Average of Average Per Prediction Episode Error =  49.27288693796237  (Avg. over 100 episodes)\n",
            "\n",
            "1300  episodes completed... out of  1500 ****************************************\n",
            " Current Learning Rate and Exploration Factor\n",
            "   Current Learning Rate - alpha (𝛼):  0.0003\n",
            "   Current Exploration Factor - epsilon (𝜀):  0.0770043145805155 \n",
            "\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  205.9576923076923\n",
            "   Cumulative Average Episode Length =  92.07 / 200  ( 0.46035 )\n",
            "   Cumulative Successful Episode Count =  140 / 1300  ( 0.1076923076923077 )\n",
            "   Cumulative Average of Average Per Prediction Episode Error =  43.91624411026542  (Avg. over 1300 episodes)\n",
            "\n",
            " Batch Statistics (episodes  1201  to  1300 ):\n",
            "   Batch Average Reward =  202.33\n",
            "   Batch Average Episode Length =  93.73 / 200  ( 0.46865 )\n",
            "   Batch Successful Episode Count =  12 / 100  ( 0.12 )\n",
            "   Batch Average of Average Per Prediction Episode Error =  48.100675845240794  (Avg. over 100 episodes)\n",
            "\n",
            "1400  episodes completed... out of  1500 ****************************************\n",
            " Current Learning Rate and Exploration Factor\n",
            "   Current Learning Rate - alpha (𝛼):  0.0003\n",
            "   Current Exploration Factor - epsilon (𝜀):  0.07547192872036323 \n",
            "\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  206.22571428571428\n",
            "   Cumulative Average Episode Length =  92.27571428571429 / 200  ( 0.4613785714285714 )\n",
            "   Cumulative Successful Episode Count =  151 / 1400  ( 0.10785714285714286 )\n",
            "   Cumulative Average of Average Per Prediction Episode Error =  44.12819729412608  (Avg. over 1400 episodes)\n",
            "\n",
            " Batch Statistics (episodes  1301  to  1400 ):\n",
            "   Batch Average Reward =  209.71\n",
            "   Batch Average Episode Length =  94.95 / 200  ( 0.47475 )\n",
            "   Batch Successful Episode Count =  11 / 100  ( 0.11 )\n",
            "   Batch Average of Average Per Prediction Episode Error =  46.88358868431497  (Avg. over 100 episodes)\n",
            "\n",
            "1500  episodes completed... out of  1500 ****************************************\n",
            " Current Learning Rate and Exploration Factor\n",
            "   Current Learning Rate - alpha (𝛼):  0.0003\n",
            "   Current Exploration Factor - epsilon (𝜀):  0.073970037338828 \n",
            "\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  205.05666666666667\n",
            "   Cumulative Average Episode Length =  92.62066666666666 / 200  ( 0.4631033333333333 )\n",
            "   Cumulative Successful Episode Count =  161 / 1500  ( 0.10733333333333334 )\n",
            "   Cumulative Average of Average Per Prediction Episode Error =  43.657518200689736  (Avg. over 1500 episodes)\n",
            "\n",
            " Batch Statistics (episodes  1401  to  1500 ):\n",
            "   Batch Average Reward =  188.69\n",
            "   Batch Average Episode Length =  97.45 / 200  ( 0.48725 )\n",
            "   Batch Successful Episode Count =  10 / 100  ( 0.1 )\n",
            "   Batch Average of Average Per Prediction Episode Error =  37.068010892580816  (Avg. over 100 episodes)\n",
            "\n",
            "---------------------------------------------------------------------------------\n",
            "𝐓𝐑𝐀𝐈𝐍𝐈𝐍𝐆 𝐂𝐎𝐌𝐏𝐋𝐄𝐓𝐄\n",
            "ᴛᴅ-ᴄᴏɴᴛʀᴏʟ ᴍᴇᴛʜᴏᴅ:\n",
            "  n-SARSA (n = 2)\n",
            "\n",
            "ᴘᴀʀᴀᴍᴇᴛᴇʀꜱ:\n",
            " Final Learning rate (𝜶) =  0.0003\n",
            " Discount Factor (𝜸) =  0.94\n",
            " Final 𝜀-greedy constant (𝜺) =  0.073970037338828\n",
            "\n",
            "ꜱᴜᴍᴍᴀʀʏ ꜱᴛᴀᴛɪꜱᴛɪᴄꜱ:\n",
            " Cumulative Average Episode Length =  92.62066666666666 / 200  ( 0.4631033333333333 )\n",
            " Cumulative Average Successful Episode Length:  200.0 / 200  ( 1.0 )\n",
            " Cumulative Successful Episode Count =  161 / 1500  ( 0.10733333333333334 )\n",
            " Cumulative Average of Average Per Prediction Episode Error =  43.657518200689736  (Avg. over 1500 episodes)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aXsfiVp5PXH"
      },
      "source": [
        "##**SARSA(n) Performance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuvVifr-kZ2q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f86b587-7472-46a1-ff93-c87d65a3385f"
      },
      "source": [
        "CartPole_TEST(model_SARSA_N, trial_count = 10, episode_count = 100, eps = 0.1, display_trial_readout = True, display_episode_readout = False)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*****Trial  1  ( 100 Episodes) *****************\n",
            "Successful Episodes =  19 / 100\n",
            "Fraction of Successful Episodes =  0.19\n",
            "Average Successful Episode Length =  200.0 / 200\n",
            "Fraction Successful Episode Length =  1.0\n",
            "Average Episode Length =  127.43 / 200\n",
            "Fractional Average Episode Length =  0.63715 \n",
            "\n",
            "*****Trial  2  ( 100 Episodes) *****************\n",
            "Successful Episodes =  26 / 100\n",
            "Fraction of Successful Episodes =  0.26\n",
            "Average Successful Episode Length =  200.0 / 200\n",
            "Fraction Successful Episode Length =  1.0\n",
            "Average Episode Length =  135.17 / 200\n",
            "Fractional Average Episode Length =  0.67585 \n",
            "\n",
            "*****Trial  3  ( 100 Episodes) *****************\n",
            "Successful Episodes =  19 / 100\n",
            "Fraction of Successful Episodes =  0.19\n",
            "Average Successful Episode Length =  200.0 / 200\n",
            "Fraction Successful Episode Length =  1.0\n",
            "Average Episode Length =  131.65 / 200\n",
            "Fractional Average Episode Length =  0.65825 \n",
            "\n",
            "*****Trial  4  ( 100 Episodes) *****************\n",
            "Successful Episodes =  31 / 100\n",
            "Fraction of Successful Episodes =  0.31\n",
            "Average Successful Episode Length =  200.0 / 200\n",
            "Fraction Successful Episode Length =  1.0\n",
            "Average Episode Length =  141.9 / 200\n",
            "Fractional Average Episode Length =  0.7095 \n",
            "\n",
            "*****Trial  5  ( 100 Episodes) *****************\n",
            "Successful Episodes =  24 / 100\n",
            "Fraction of Successful Episodes =  0.24\n",
            "Average Successful Episode Length =  200.0 / 200\n",
            "Fraction Successful Episode Length =  1.0\n",
            "Average Episode Length =  139.24 / 200\n",
            "Fractional Average Episode Length =  0.6962 \n",
            "\n",
            "*****Trial  6  ( 100 Episodes) *****************\n",
            "Successful Episodes =  26 / 100\n",
            "Fraction of Successful Episodes =  0.26\n",
            "Average Successful Episode Length =  200.0 / 200\n",
            "Fraction Successful Episode Length =  1.0\n",
            "Average Episode Length =  140.45 / 200\n",
            "Fractional Average Episode Length =  0.7022499999999999 \n",
            "\n",
            "*****Trial  7  ( 100 Episodes) *****************\n",
            "Successful Episodes =  32 / 100\n",
            "Fraction of Successful Episodes =  0.32\n",
            "Average Successful Episode Length =  200.0 / 200\n",
            "Fraction Successful Episode Length =  1.0\n",
            "Average Episode Length =  139.0 / 200\n",
            "Fractional Average Episode Length =  0.695 \n",
            "\n",
            "*****Trial  8  ( 100 Episodes) *****************\n",
            "Successful Episodes =  27 / 100\n",
            "Fraction of Successful Episodes =  0.27\n",
            "Average Successful Episode Length =  200.0 / 200\n",
            "Fraction Successful Episode Length =  1.0\n",
            "Average Episode Length =  139.64 / 200\n",
            "Fractional Average Episode Length =  0.6981999999999999 \n",
            "\n",
            "*****Trial  9  ( 100 Episodes) *****************\n",
            "Successful Episodes =  29 / 100\n",
            "Fraction of Successful Episodes =  0.29\n",
            "Average Successful Episode Length =  200.0 / 200\n",
            "Fraction Successful Episode Length =  1.0\n",
            "Average Episode Length =  135.17 / 200\n",
            "Fractional Average Episode Length =  0.67585 \n",
            "\n",
            "*****Trial  10  ( 100 Episodes) *****************\n",
            "Successful Episodes =  30 / 100\n",
            "Fraction of Successful Episodes =  0.3\n",
            "Average Successful Episode Length =  200.0 / 200\n",
            "Fraction Successful Episode Length =  1.0\n",
            "Average Episode Length =  145.4 / 200\n",
            "Fractional Average Episode Length =  0.727 \n",
            "\n",
            "CUMULATIVE RESULTS:\n",
            "Total Number of Successful Episodes =  263 / 1000\n",
            "Fraction of Successful Episodes =  0.263\n",
            "Average of Average Successful Episode Steps per Trial =  200.0 / 200\n",
            "Average of Average Successful Episode Steps per Trial (Fractional) =  1.0\n",
            "Average of Average Successful Episode Steps per Trial =  137.50500000000002 / 200\n",
            "Average of Average Episode Lengths per Trial (Fractional) =  0.6875250000000002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBijp_cs1gFS"
      },
      "source": [
        "##**CartPole Single Trial Results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0WPu-8ik0Za"
      },
      "source": [
        "env.reset()\n",
        "\n",
        "input_model = model_SARSA_0\n",
        "\n",
        "for i in range(50000):\n",
        "  action = action_selection_NN(env, env.env.state, input_model, eps = 0.0)\n",
        "  print('state = ', env.env.state)\n",
        "\n",
        "  print(\"step i\",i,\"action=\",action)\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  print(\"obs=\",obs,\"reward=\",reward,\"done=\",done,\"info=\",info)\n",
        "\n",
        "  print('tau: ', env.env.tau)\n",
        "  \n",
        "\n",
        "\n",
        "  if done:\n",
        "    break\n",
        "\n",
        "\n",
        "env.close()\n",
        "print(\"Iterations that were run:\", i + 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-D6sAWBrPS6"
      },
      "source": [
        "env.reset()\n",
        "prev_screen = env.render(mode='rgb_array')\n",
        "plt.imshow(prev_screen)\n",
        "\n",
        "input_model = model_SARSA_0\n",
        "\n",
        "for i in range(50000):\n",
        "  action = action_selection_NN(env, env.env.state, input_model, eps = 0.0)\n",
        "  print('state = ', env.env.state)\n",
        "\n",
        "  print(\"step i\",i,\"action=\",action)\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  print(\"obs=\",obs,\"reward=\",reward,\"done=\",done,\"info=\",info)\n",
        "  screen = env.render(mode='rgb_array')\n",
        "  print('tau: ', env.env.tau)\n",
        "  \n",
        "  plt.imshow(screen)\n",
        "  ipythondisplay.clear_output(wait=True)\n",
        "  ipythondisplay.display(plt.gcf())\n",
        "\n",
        "  if done:\n",
        "    break\n",
        "\n",
        "ipythondisplay.clear_output(wait=True)\n",
        "env.close()\n",
        "print(\"Iterations that were run:\", i + 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdGkwUHWmbuv"
      },
      "source": [
        "#**VII. Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3kWHZyA1u6C"
      },
      "source": [
        "Overall function approximation works quite well and is a practical alternative to tabular methods. However I find that results are far more sensitive to hyperparameter tuning compared to what was experienced with tabular methods."
      ]
    }
  ]
}